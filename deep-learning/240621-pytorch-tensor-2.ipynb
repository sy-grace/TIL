{"cells":[{"cell_type":"markdown","metadata":{"id":"UKjg766IvMdI"},"source":["# 텐서 조작 (2)"]},{"cell_type":"markdown","metadata":{"id":"OXUZV0j3nBJd"},"source":["## 실습 개요\n","\n","1) **실습 목적**\n","\n","\n","이번 실습은 이론으로 배웠던 **PyTorch의 조작법**을 코드를 통해 확인하고 스스로 실행해 볼 수 있도록 구성하였습니다. 여러 가지 조작 **예시**들과 그에 따른 **결과**를 통해 PyTorch의 조작법을 쉽게 익힐 수 있습니다.😊\n","\n","\n","2) **수강 목표**\n","\n","- 텐서의 수학적인 연산과 broadcasting 에 대한 개념을 이해하고 실현할 수 있다. (<font color=red><b>Mathmatical Operations and Broadcasting</font></b>)\n","- Sparse Tensor의 조작법을 익힐 수 있다. (<font color=red><b>Sparse Tensor</font></b>)"]},{"cell_type":"markdown","metadata":{"id":"H5CYPnoeDoRu"},"source":["### 실습 목차\n","* 1. 텐서 연산 및 조작\n","  * 1-1. 텐서 간의 계산 실습\n","  * 1-2. Broadcasting 을 이용한 텐서 값 변경\n","  * 1-3. Broadcasting 을 이용한 차원이 다른 텐서 간의 계산 실습\n","* 2. Sparse Tensor 조작 및 실습\n","  * 2-1. COO Tensor 에 대한 이해 및 실습\n","  * 2-2. CSC/CSR Tensor 에 대한 이해 및 실습\n","  * 2-3. Sparse Tensor의 필요성 이해 및 실습\n","  * 2-4. Sparse Tensor 의 조작 예시"]},{"cell_type":"markdown","metadata":{"id":"tMbQ5FwAX2Bl"},"source":["### 환경 설정\n","> PyTorch 설치 및 불러오기\n","\n","<font color = blue><b>\n","- 패키지 설치 및 임포트\n","</font><b>"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"OW0tn1H4WVQA"},"outputs":[],"source":["import torch # PyTorch 불러오기\n","import numpy as np # numpy 불러오기\n","import warnings # 경고 문구 제거\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"EZ118tS70iHF"},"source":["## 1. 텐서 연산 및 조작\n","\n","```\n","💡 목차 개요 : 텐서의 연산을 이해하고, 연산 과정 중 broadcasting이 어떻게 적용되는지 이해하고 실습한다.\n","```\n","\n","- 1-1. 텐서 간의 계산 실습\n","- 1-2. Broadcasting 을 이용한 텐서 값 변경\n","- 1-3. Broadcasting 을 이용한 차원이 다른 텐서 간의 계산 실습\n"]},{"cell_type":"markdown","metadata":{"id":"ldTE5o1A1BWI"},"source":["### 1-1 텐서 간의 계산 실습\n","> 텐서 간의 계산과 텐서 내의 계산 과정을 알아봅니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cun6rzXl1VXd"},"source":["#### 📝 설명 : 텐서 간의 사칙연산\n","* add : 텐서 간의 덧셈을 수행합니다. (+)\n","    * torch.add(a, b)\n","    * a.add(b)\n","    * a + b\n","* sub : 텐서 간의 뺄셈을 수행합니다. (-)\n","    * torch.sub(a, b)\n","    * a.sub(b)\n","    * a - b\n","* mul : 텐서 간의 곱셈을 수행합니다. (*)\n","    * torch.mul(a, b)\n","    * a.mul(b)\n","    * a * b\n","* div : 텐서 간의 나눗셈을 수행합니다. (/)\n","    * torch.div(a, b)\n","    * a.div(b)\n","    * a / b\n","\n","📚 참고할만한 자료:\n","* [add] https://pytorch.org/docs/stable/generated/torch.add.html\n","* [sub] https://pytorch.org/docs/stable/generated/torch.add.html\n","* [mul] https://pytorch.org/docs/stable/generated/torch.add.html\n","* [div] https://pytorch.org/docs/stable/generated/torch.add.html\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":381,"status":"ok","timestamp":1689149084807,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"_WZBms6n10BJ","outputId":"bb26b457-e856-4327-c249-465b91a722b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["덧셈\n","a+b : \n"," tensor([[ 3, -3],\n","        [ 5,  4]])\n","\n","\n","torch.add(a,b) : \n"," tensor([[ 3, -3],\n","        [ 5,  4]])\n","------------------------------\n","뺄셈\n","a-b : \n"," tensor([[-1,  1],\n","        [-1,  2]])\n","\n","\n","torch.sub(a,b) : \n"," tensor([[-1,  1],\n","        [-1,  2]])\n","------------------------------\n","곱셈\n","a*b : \n"," tensor([[2, 2],\n","        [6, 3]])\n","\n","\n","torch.mul(a,b) : \n"," tensor([[2, 2],\n","        [6, 3]])\n","------------------------------\n","나눗셈\n","a/b : \n"," tensor([[0.5000, 0.5000],\n","        [0.6667, 3.0000]])\n","\n","\n","torch.div(a,b) : \n"," tensor([[0.5000, 0.5000],\n","        [0.6667, 3.0000]])\n"]}],"source":["tensor_a = torch.tensor([[1, -1], [2, 3]])\n","tensor_b = torch.tensor([[2, -2] ,[3, 1]])\n","\n","print('덧셈')\n","print(\"a+b : \\n\", tensor_a + tensor_b)\n","print('\\n')\n","print(\"torch.add(a,b) : \\n\", torch.add(tensor_a, tensor_b))\n","\n","print('---'*10)\n","\n","print('뺄셈')\n","print(\"a-b : \\n\", tensor_a - tensor_b)\n","print('\\n')\n","print(\"torch.sub(a,b) : \\n\", torch.sub(tensor_a, tensor_b))\n","\n","print('---'*10)\n","\n","print('곱셈')\n","print(\"a*b : \\n\", tensor_a * tensor_b)\n","print('\\n')\n","print(\"torch.mul(a,b) : \\n\", torch.mul(tensor_a, tensor_b))\n","\n","print('---'*10)\n","\n","print('나눗셈')\n","print(\"a/b : \\n\", tensor_a / tensor_b)\n","print('\\n')\n","print(\"torch.div(a,b) : \\n\", torch.div(tensor_a, tensor_b))"]},{"cell_type":"markdown","metadata":{"id":"ov9gedcy3Lug"},"source":["#### 📝 설명 : 텐서의 통계치\n","함수의 dim 파라미터 값에 따라 결과가 달라지는 것을 유의하세요❗\n","* sum : 텐서의 원소들의 합을 반환\n","\n","📚 참고할만한 자료:\n","* [sum] : https://pytorch.org/docs/stable/generated/torch.sum.html"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":434,"status":"ok","timestamp":1688752385720,"user":{"displayName":"JaeHyun Lee","userId":"13762872260843906568"},"user_tz":-540},"id":"b6hQPfp9bF09","outputId":"155890d0-958f-4908-97c4-c2718a7e2734"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1, 2],\n","        [3, 4]])\n","Shape :  torch.Size([2, 2])\n","\n","\n","dimension 지정 안했을 때 :  tensor(10)\n","dim = 0 일 때 :  tensor([4, 6])\n","dim = 1 일 때 :  tensor([3, 7])\n"]}],"source":["tensor_a = torch.tensor([[1, 2], [3, 4]])\n","print(tensor_a)\n","print(\"Shape : \", tensor_a.size())\n","\n","print('\\n')\n","\n","print(\"dimension 지정 안했을 때 : \", torch.sum(tensor_a))  # 모든 원소의 합을 반환 함\n","print(\"dim = 0 일 때 : \", torch.sum(tensor_a, dim=0))  # 행을 기준 (행 인덱스 변화)으로 합함 (0행 0열 + 1행 0열, 0행 1열 + 1행 1열)\n","print(\"dim = 1 일 때 : \", torch.sum(tensor_a, dim=1)) # 열을 기준 (열 인덱스 변화)으로 합함 (0행 0열 + 0행 1열, 1행 0열 + 1행 1열)"]},{"cell_type":"markdown","metadata":{"id":"BMWbODjMcnbn"},"source":["#### 📝 설명 : 텐서의 통계치\n","함수의 dim 파라미터 값에 따라 결과가 달라지는 것을 유의하세요❗\n","* mean : 텐서의 원소들의 평균을 반환\n","📚 참고할만한 자료:\n","* [mean] : https://pytorch.org/docs/stable/generated/torch.mean.html"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":378,"status":"ok","timestamp":1688752387988,"user":{"displayName":"JaeHyun Lee","userId":"13762872260843906568"},"user_tz":-540},"id":"V7HMd6sPeBhM","outputId":"f57e74de-6650-4b9f-9f4b-7f83b521353a"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1., 2.],\n","        [3., 4.]])\n","Shape :  torch.Size([2, 2])\n","\n","\n","dimension 지정 안했을 때 :  tensor(2.5000)\n","dim = 0 일 때 :  tensor([2., 3.])\n","dim = 1 일 때 :  tensor([1.5000, 3.5000])\n"]}],"source":["tensor_a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) # mean 은 실수가 나올 수 있으므로 float 로 지정해주어야 함.\n","print(tensor_a)\n","print(\"Shape : \", tensor_a.size())\n","\n","print('\\n')\n","\n","print(\"dimension 지정 안했을 때 : \", torch.mean(tensor_a))  # 모든 원소의 평균을 반환 함\n","print(\"dim = 0 일 때 : \", torch.mean(tensor_a, dim=0))  # 행을 기준 (행 인덱스 변화)으로 평균 구함 ((0행 0열 + 1행 0열)/2, (0행 1열 + 1행 1열)/2)\n","print(\"dim = 1 일 때 : \", torch.mean(tensor_a, dim=1)) # 열을 기준 (열 인덱스 변화)으로 평균 구함 ((0행 0열 + 0행 1열)/2, (1행 0열 + 1행 1열)/2)"]},{"cell_type":"markdown","metadata":{"id":"sqp5T3a3c_yg"},"source":["#### 📝 설명 : 텐서의 통계치\n","함수의 dim 파라미터 값에 따라 결과가 달라지는 것을 유의하세요❗\n","* max : 텐서의 원소들의 가장 큰 값을 반환\n","* min : 텐서의 원소들의 가장 작은 값을 반환\n","\n","📚 참고할만한 자료:\n","* [max] : https://pytorch.org/docs/stable/generated/torch.max.html\n","* [min] : https://pytorch.org/docs/stable/generated/torch.min.html"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":283,"status":"ok","timestamp":1688752390565,"user":{"displayName":"JaeHyun Lee","userId":"13762872260843906568"},"user_tz":-540},"id":"6sdl95xIek7r","outputId":"5d14d64f-99ef-4ac9-b54e-2b6ad11d8cda"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1, 2],\n","        [3, 4]])\n","Shape :  torch.Size([2, 2])\n","\n","\n","dimension 지정 안했을 때 :  tensor(4)\n","dim = 0 일 때 :  tensor([3, 4])\n","dim = 1 일 때 :  tensor([2, 4])\n","\n","\n","dimension 지정 안했을 때 :  tensor(1)\n","dim = 0 일 때 :  tensor([1, 2])\n","dim = 1 일 때 :  tensor([1, 3])\n"]}],"source":["import torch\n","tensor_a = torch.tensor([[1, 2], [3, 4]])\n","print(tensor_a)\n","print(\"Shape : \", tensor_a.size())\n","print('\\n')\n","\n","print(\"dimension 지정 안했을 때 : \", torch.max(tensor_a))  # 모든 원소 중 최댓값 반환\n","print(\"dim = 0 일 때 : \", torch.max(tensor_a, dim=0).values)  # 행을 기준 (행 인덱스 변화)으로 max 비교 (max(0행 0열 , 1행 0열), max(0행 1열 , 1행 1열))\n","print(\"dim = 1 일 때 : \", torch.max(tensor_a, dim=1).values) # 열을 기준 (열 인덱스 변화)으로 max 비교 (max(0행 0열 , 0행 1열), max(1행 0열 , 1행 1열))\n","print('\\n')\n","\n","print(\"dimension 지정 안했을 때 : \", torch.min(tensor_a))  # 모든 원소의 최솟값 반환 함\n","print(\"dim = 0 일 때 : \", torch.min(tensor_a, dim=0).values)  # 행을 기준 (행 인덱스 변화)으로 min 비교 (min(0행 0열 , 1행 0열), min(0행 1열 , 1행 1열))\n","print(\"dim = 1 일 때 : \", torch.min(tensor_a, dim=1).values) # 열을 기준 (열 인덱스 변화)으로 min 비교 (min(0행 0열 , 0행 1열), min(1행 0열 , 1행 1열))"]},{"cell_type":"markdown","metadata":{"id":"E6iJItkFdT9F"},"source":["#### 📝 설명 : 텐서의 통계치\n","함수의 dim 파라미터 값에 따라 결과가 달라지는 것을 유의하세요❗\n","* argmax : 텐서의 원소들의 가장 큰 값의 **위치** 반환\n","* argmin : 텐서의 원소들의 가장 작은 값의 **위치** 반환\n","\n","📚 참고할만한 자료:\n","* [argmax] : https://pytorch.org/docs/stable/generated/torch.argmax.html\n","* [argmin] : https://pytorch.org/docs/stable/generated/torch.argmin.html"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":448,"status":"ok","timestamp":1688752393199,"user":{"displayName":"JaeHyun Lee","userId":"13762872260843906568"},"user_tz":-540},"id":"MTJSU8rldTH2","outputId":"8db8db6d-d285-41a9-88e0-9309064b7b9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1, 2],\n","        [3, 4]])\n","Shape :  torch.Size([2, 2])\n","\n","\n","dimension 지정 안했을 때 :  tensor(3)\n","dim = 0 일 때 :  tensor([1, 1])\n","dim = 1 일 때 :  tensor([1, 1])\n","\n","\n","dimension 지정 안했을 때 :  tensor(0)\n","dim = 0 일 때 :  tensor([0, 0])\n","dim = 1 일 때 :  tensor([0, 0])\n"]}],"source":["tensor_a = torch.tensor([[1, 2], [3, 4]])\n","print(tensor_a)\n","print(\"Shape : \",tensor_a.size())\n","print('\\n')\n","\n","print(\"dimension 지정 안했을 때 : \", torch.argmax(tensor_a))  # 모든 원소 중 최댓값 위치 반환함\n","print(\"dim = 0 일 때 : \", torch.argmax(tensor_a, dim=0))  # 행을 기준 (행 인덱스 변화)으로 max 비교 (max(0행 0열 , 1행 0열), max(0행 1열 , 1행 1열)) => 위치 반환\n","print(\"dim = 1 일 때 : \", torch.argmax(tensor_a, dim=1)) # 열을 기준 (열 인덱스 변화)으로 max 비교 (max(0행 0열 , 0행 1열), max(1행 0열 , 1행 1열)) => 위치 반환\n","\n","print('\\n')\n","\n","print(\"dimension 지정 안했을 때 : \", torch.argmin(tensor_a))  # 모든 원소의 최솟값 위치 반환 함\n","print(\"dim = 0 일 때 : \", torch.argmin(tensor_a, dim=0))  # 행을 기준 (행 인덱스 변화)으로 min 비교 (min(0행 0열 , 1행 0열), min(0행 1열 , 1행 1열)) => 위치 반환\n","print(\"dim = 1 일 때 : \", torch.argmin(tensor_a, dim=1)) # 열을 기준 (열 인덱스 변화)으로 min 비교 (min(0행 0열 , 0행 1열), min(1행 0열 , 1행 1열)) => 위치 반환"]},{"cell_type":"markdown","metadata":{"id":"KZmeMd3agwBP"},"source":["#### 📝 설명 : 행렬 및 벡터 계산\n","* dot : **벡터**의 내적 (inner product) 반환\n","  * torch.dot(a,b)\n","  * a.dot(b)\n","\n","📚 참고할만한 자료:\n","* [dot] : https://pytorch.org/docs/stable/generated/torch.dot.html"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":291,"status":"ok","timestamp":1688752396024,"user":{"displayName":"JaeHyun Lee","userId":"13762872260843906568"},"user_tz":-540},"id":"rn2ZpS_whLTP","outputId":"a467651d-5e1c-4770-bf1f-2a52e8954f4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["v1.dot(u1) :  tensor(11)\n","torch.dot(v1, u1) :  tensor(11)\n"]}],"source":["v1 = torch.tensor([1, 2])\n","u1 = torch.tensor([3, 4])\n","\n","print(\"v1.dot(u1) : \", v1.dot(u1)) # v1 과 u1 내적 (torch.tensor 에도 dot 함수 존재)\n","print(\"torch.dot(v1, u1) : \", torch.dot(v1, u1)) # v1 과 u1 내적"]},{"cell_type":"markdown","metadata":{"id":"hmDmTIy4hx1M"},"source":["#### 📝 설명 : 행렬 및 벡터 계산\n","* matmul : 두 텐서 간의 행렬곱 반환 ***※ 원소 곱과 다름 주의❗***\n","  * torch.matmul(a,b)\n","  * a.matmul(b)\n","\n","📚 참고할만한 자료:\n","* [matmul] : https://pytorch.org/docs/stable/generated/torch.matmul.html"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1689149537639,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"aBADhk_tiIJO","outputId":"01ab4ebf-5dba-407d-d51a-2aec7bf63d2c"},"outputs":[{"name":"stdout","output_type":"stream","text":["A:  tensor([[1, 2],\n","        [3, 4]])\n","B:  tensor([[-1,  2],\n","        [ 1,  0]])\n","\n","\n","AB :  tensor([[1, 2],\n","        [1, 6]])\n","BA :  tensor([[5, 6],\n","        [1, 2]])\n"]}],"source":["A = torch.tensor([[1, 2], [3, 4]])  # (2,2) Tensor\n","B = torch.tensor([[-1, 2], [1, 0]])  # (2,2) Tensor\n","print(\"A: \", A)\n","print(\"B: \", B)\n","\n","print('\\n')\n","\n","print(\"AB : \", torch.matmul(A, B)) # A에서 B를 행렬곱\n","print(\"BA : \", B.matmul(A))  # B에서 A를 행렬곱"]},{"cell_type":"markdown","metadata":{"id":"DIifbNzskKm8"},"source":["### 1-2. Broadcasting 을 이용한 텐서 값 변경\n","> Broadcasting 을 이용하여 텐서의 원소를 변경하는 방법에 대해 이해하고 실습합니다."]},{"cell_type":"markdown","metadata":{"id":"hp7PE4F_kNvW"},"source":["#### 📝 설명 : Broadcasting 을 이용한 텐서 원소 변경\n","* scalar 값으로 텐서 원소 변경하기\n","  * Indexing으로 텐서 원소에 접근 후 scalar 값으로 원소 변경\n","\n","📚 참고할만한 자료:\n","* [Broadcasing semantics] : https://pytorch.org/docs/stable/notes/broadcasting.html"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":306,"status":"ok","timestamp":1689149701311,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"0yapVKG4lLqX","outputId":"30874c17-fe9d-45b6-d576-498cd5ef514e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original : \n"," tensor([[-1.2763, -0.6511],\n","        [-0.2358,  0.0203],\n","        [-1.2104, -0.9944]])\n","\n","\n","변경된 텐서 : \n"," tensor([[10.0000, 10.0000],\n","        [-0.2358,  0.0203],\n","        [-1.2104, -0.9944]])\n"]}],"source":["tensor_a = torch.randn(3, 2)\n","print(\"Original : \\n\", tensor_a)\n","\n","print('\\n')\n","\n","## 0 행의 모든 열을 10 으로 변경하기\n","tensor_a[0, :] = 10 # 0행의 모든 열에 broadcasting 을 통한 scalar 값 대입\n","print(\"변경된 텐서 : \\n\", tensor_a)"]},{"cell_type":"markdown","metadata":{"id":"ryUS3yIOnA-g"},"source":["#### 📝 설명 : Broadcasting 을 이용한 텐서 원소 변경\n","* 텐서 값으로 텐서 원소 변경하기\n","  * Indexing으로 텐서 원소에 접근 후 텐서 값으로 원소 변경\n","\n","📚 참고할만한 자료:\n","* [Broadcasing semantics] : https://pytorch.org/docs/stable/notes/broadcasting.html"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":314,"status":"ok","timestamp":1689149746296,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"lMQbXVrynEPp","outputId":"52fde8fe-e8b7-4af7-824b-b576ff76d07f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original : \n"," tensor([[-0.4640,  0.6063],\n","        [ 0.1150,  2.0040],\n","        [-0.3974, -0.8116]])\n","\n","\n","변경된 Tensor : \n"," tensor([[0., 1.],\n","        [0., 1.],\n","        [0., 1.]])\n"]}],"source":["tensor_a = torch.randn(3, 2)\n","print(\"Original : \\n\", tensor_a)\n","\n","print('\\n')\n","\n","## 모든 값을 tensor [0,1]로 변경하기\n","tensor_a[:, :] = torch.tensor([0, 1]) # 모든 값에 접근하여 [0,1] 로 변경\n","print(\"변경된 Tensor : \\n\", tensor_a)"]},{"cell_type":"markdown","metadata":{"id":"_7ou_M9do-Zd"},"source":["### 1-3. Broadcasting 을 이용한 차원이 다른 텐서 간의 계산 실습\n","> Broadcasting 을 이용하여 차원이 다른 텐서 간의 계산 방식에 대해 이해하고 실습합니다."]},{"cell_type":"markdown","metadata":{"id":"7AHbq1lgpgtd"},"source":["#### 📝 설명 : Broadcasting 을 이용한 계산\n","* 차원이 서로 다른 텐서 간의 계산을 broadcasting 을 통해 할 수 있습니다.\n","\n","📚 참고할만한 자료:\n","* [Broadcasing semantics] : https://pytorch.org/docs/stable/notes/broadcasting.html"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":323,"status":"ok","timestamp":1689149822271,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"rM5KMdw6pqxq","outputId":"823ed4f0-bc94-4060-e26e-683d36b63390"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor A : \n"," tensor([[1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.]])\n","\n","\n","Tensor B : \n"," tensor([1, 2, 3])\n","\n","\n","A + B : \n"," tensor([[2., 2., 3.],\n","        [1., 3., 3.],\n","        [1., 2., 4.]])\n"]}],"source":["tensor_a = torch.eye(3)\n","print(\"Tensor A : \\n\",tensor_a)\n","\n","print('\\n')\n","\n","tensor_b = torch.tensor([1, 2, 3])\n","print(\"Tensor B : \\n\", tensor_b)\n","\n","print('\\n')\n","\n","print('A + B : \\n', tensor_a + tensor_b) # broadcasting을 통해 (3,) 인 B가 (3,3)으로 변환되어 계산 (행의 확장)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1689149861027,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"vp7Q-Ub7qmMi","outputId":"b14f4b7d-f36a-420e-808f-1fde8e3871f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor A : \n"," tensor([[1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.]])\n","\n","\n","Tensor B : \n"," tensor([[1],\n","        [2],\n","        [3]])\n","\n","\n","A + B : \n"," tensor([[2., 1., 1.],\n","        [2., 3., 2.],\n","        [3., 3., 4.]])\n"]}],"source":["tensor_a = torch.eye(3)\n","print(\"Tensor A : \\n\", tensor_a)\n","\n","print('\\n')\n","\n","tensor_b = torch.tensor([1, 2, 3]).reshape(3, 1) # 행 벡터로 형식으로 변환\n","print(\"Tensor B : \\n\", tensor_b)\n","\n","print('\\n')\n","\n","print('A + B : \\n', tensor_a + tensor_b) # broadcasting을 통해 (3,1) 인 B가 (3,3)으로 변환되어 계산 (열의 확장)"]},{"cell_type":"markdown","metadata":{"id":"5J0V1R7rrFwj"},"source":["#### 📝 설명 : Broadcasting 을 이용한 계산\n","* 차원의 맞지 않는 경우, 차원을 추가하여 broadcasting 으로 텐서 간의 계산을 할 수 있습니다.\n","\n","📚 참고할만한 자료:\n","* [Broadcasing semantics] : https://pytorch.org/docs/stable/notes/broadcasting.html"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":266},"executionInfo":{"elapsed":4,"status":"error","timestamp":1689149935285,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"3qlOVCdFrVQq","outputId":"ebadc45b-1a34-46e0-d2ee-56aa3d97866c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor size : torch.Size([3, 2, 5]), mean size : torch.Size([3, 2])\n","\n","\n"]},{"ename":"RuntimeError","evalue":"The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 2","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor size : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor_a\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, mean size : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_a\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtensor_a\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean_a\u001b[49m)  \u001b[38;5;66;03m# 에러 발생! 차원이 달라서 계산이 되지 않음\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 2"]}],"source":["tensor_a = torch.randn(3, 2, 5)\n","mean_a = tensor_a.mean(2) # 열 기준 평균값\n","print(f\"Tensor size : {tensor_a.size()}, mean size : {mean_a.size()}\")\n","\n","print('\\n')\n","\n","print(tensor_a - mean_a)  # 에러 발생! 차원이 달라서 계산이 되지 않음"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":326,"status":"ok","timestamp":1689149971280,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"k9yknx4MtJ_6","outputId":"f7005a0b-33ff-455c-c391-b26fd6ed3567"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 2, 1])\n","\n","\n","tensor([[[ 2.3126e-02,  4.4009e-01, -3.8365e-01,  6.2486e-01, -7.0441e-01],\n","         [ 5.0169e-01, -2.3717e-02, -6.5864e-01, -2.6497e-02,  2.0716e-01]],\n","\n","        [[ 8.1608e-01,  3.4825e-01,  7.3040e-01, -1.8974e+00,  2.6794e-03],\n","         [ 1.1102e-01, -5.6161e-01, -1.6277e-01,  8.1305e-01, -1.9969e-01]],\n","\n","        [[-8.4080e-01,  1.7350e+00,  3.0839e-01, -4.6323e-01, -7.3932e-01],\n","         [-3.3951e-01,  1.0937e+00, -2.8075e+00,  1.5205e+00,  5.3273e-01]]])\n"]}],"source":["# 차원 생성 후 broadcasting\n","unseq_mean = mean_a.unsqueeze(-1) # 마지막 축 추가\n","print(unseq_mean.size())\n","\n","print('\\n')\n","\n","print(tensor_a - unseq_mean)"]},{"cell_type":"markdown","metadata":{"id":"OYUG6flzvSmM"},"source":["## 2. Sparse Tensor 조작 및 실습\n","\n","```\n","💡 목차 개요 : 다양한 종류의 Sparse Tensor 를 생성하는 방법을 알아보고, 이를 조작할 수 있는 방법에 대해 알아봅니다.\n","```\n","\n","- 2-1. COO Tensor 에 대한 이해 및 실습\n","- 2-2. CSC/CSR Tensor 에 대한 이해 및 실습\n","- 2-3. Sparse Tensor의 필요성 이해 및 실습\n","- 2-4. Sparse Tensor 의 조작 예시\n"]},{"cell_type":"markdown","metadata":{"id":"f3-Lx6q2vd_j"},"source":["### 2-1 COO Sparse Tensor에 대한 실습\n","\n","> Sparse tensor 로 변환하는 방법 중 COO 방식에 대해 알아보고 실습합니다."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":343,"status":"ok","timestamp":1689150083919,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"N4nXauTAvvfg","outputId":"3ff62a3c-e251-49d2-d717-16ca2fc59cf5"},"outputs":[{"data":{"text/plain":["tensor(indices=tensor([[0, 1],\n","                       [1, 0]]),\n","       values=tensor([2., 3.]),\n","       size=(2, 2), nnz=2, layout=torch.sparse_coo)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["a = torch.tensor([[0, 2.], [3, 0]])\n","\n","a.to_sparse() # COO Sparse tensor 로 변환"]},{"cell_type":"markdown","metadata":{"id":"Ba1dGmhBwQdQ"},"source":["#### 📝 설명 : COO Sparse Tensor 생성\n","* sparse_coo_tensor : COO 형식의 sparse tensor 를 생성하는 함수\n","  * indices : 0 이 아닌 값을 가진 행,열의 위치\n","  * values : 0 이 아닌 값\n","  * nnz : 0 이 아닌 값의 개수\n","\n","\n","📚 참고할만한 자료:\n","* [sparse_coo_tensor] : https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":655,"status":"ok","timestamp":1689150498739,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"TCrQh-h1ymGz","outputId":"1950efd8-d345-4ed5-a0e1-dc51f3641a36"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(indices=tensor([[0, 1, 1],\n","                       [2, 0, 1]]),\n","       values=tensor([4, 5, 6]),\n","       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n","\n","\n","tensor([[0, 0, 4],\n","        [5, 6, 0]])\n"]}],"source":["indices = torch.tensor([[0, 1, 1],[2, 0, 1]]) # 0이 아닌 값의 (index, column) pairs\n","values = torch.tensor([4, 5, 6]) # 0 이 아닌 값의 values, values의 사이\n","sparse_tensor = torch.sparse_coo_tensor(indices = indices, values = values, size=(2, 3)) # (2,3)의 sparse tensor\n","\n","print(sparse_tensor)\n","print('\\n')\n","print(sparse_tensor.to_dense())"]},{"cell_type":"markdown","metadata":{"id":"gTFuYUfYv1Cm"},"source":["### 2-2 CSR/CSC Sparse Tensor에 대한 실습\n","\n","> Sparse tensor 로 변환하는 방법 중 CSR/CSC 방식에 대해 알아보고 실습합니다."]},{"cell_type":"markdown","metadata":{"id":"ewY3PfLLrd-g"},"source":["#### 📝 설명 : CSR Sparse Tensor 로 변환\n","* to_sparse_csr : Dense tensor를 CSR 형식의 Sparse tensor로 변환하는 함수\n","  * crow_indices : 0 이 아닌 값을 가진 행의 위치 (첫번째는 무조건 0)\n","  * col_indices : 0 이 아닌 값을 가진 열의 위치\n","  * values : 0 이 아닌 값\n","  * nnz : 0 이 아닌 값의 개수\n","  \n","📚 참고할만한 자료:\n","* [csr] : https://pytorch.org/docs/stable/sparse.html#sparse-csr-docs\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":275,"status":"ok","timestamp":1688752421867,"user":{"displayName":"JaeHyun Lee","userId":"13762872260843906568"},"user_tz":-540},"id":"cK-8iVBUsdY3","outputId":"5afed4a7-8010-4c45-ab62-4a35b615374b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape :  torch.Size([2, 4])\n","tensor([[0, 0, 4, 3],\n","        [5, 6, 0, 0]])\n","\n","\n"]},{"data":{"text/plain":["tensor(crow_indices=tensor([0, 2, 4]),\n","       col_indices=tensor([2, 3, 0, 1]),\n","       values=tensor([4, 3, 5, 6]), size=(2, 4), nnz=4,\n","       layout=torch.sparse_csr)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["t = torch.tensor([[0, 0, 4, 3], [5, 6, 0, 0]])\n","print(\"Shape : \", t.size())\n","print(t)\n","\n","print('\\n')\n","\n","t.to_sparse_csr()  # Dense Tensor를 CSR Sparse Tensor 형식으로 변환"]},{"cell_type":"markdown","metadata":{"id":"tvgls3yPr8Eb"},"source":["#### 📝 설명 : CSC Sparse Tensor 로 변환\n","* to_sparse_csc : Dense tensor를 CSC 형식의 Sparse tensor로 변환하는 함수\n","  * ccol_indices : 0 이 아닌 값의 열 위치 (첫번째 원소는 무조건 0)\n","  * row_indices : 0 이 아닌 값의 행 위치\n","  * values : 0 이 아닌 값들\n","  * nnz : 0 이 아닌 값의 개수\n","  \n","📚 참고할만한 자료:\n","* [csc] : https://pytorch.org/docs/stable/sparse.html#sparse-csc-docs"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":389,"status":"ok","timestamp":1688752424582,"user":{"displayName":"JaeHyun Lee","userId":"13762872260843906568"},"user_tz":-540},"id":"MZ4wlEButpT7","outputId":"c5a320a2-5a95-4171-e450-b00c21041f40"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape :  torch.Size([2, 4])\n","tensor([[0, 0, 4, 3],\n","        [5, 6, 0, 0]])\n","\n","\n"]},{"data":{"text/plain":["tensor(ccol_indices=tensor([0, 1, 2, 3, 4]),\n","       row_indices=tensor([1, 1, 0, 0]),\n","       values=tensor([5, 6, 4, 3]), size=(2, 4), nnz=4,\n","       layout=torch.sparse_csc)"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["t = torch.tensor([[0, 0, 4, 3], [5, 6, 0, 0]])\n","print(\"Shape : \", t.size())\n","print(t)\n","\n","print('\\n')\n","\n","t.to_sparse_csc()  # Dense Tensor 를 CSC Spare tensor 형식으로 변환"]},{"cell_type":"markdown","metadata":{"id":"BidxZpX-sikK"},"source":["#### 📝 설명 : CSR Sparse Tensor 생성\n","* sparse_csr_tensor : CSR 형식의 Sparse tensor 를 생성하는 함수\n","\n","📚 참고할만한 자료:\n","* [sparse_csr_tensor] : https://pytorch.org/docs/stable/generated/torch.sparse_csr_tensor.html"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297,"status":"ok","timestamp":1689150928283,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"KGQwgI2Iubt9","outputId":"1742f496-5ae3-46bb-e795-78e78631fb5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(crow_indices=tensor([0, 2, 2]),\n","       col_indices=tensor([0, 1]),\n","       values=tensor([1, 2]), size=(2, 2), nnz=2, layout=torch.sparse_csr)\n","\n","\n","tensor([[1, 2],\n","        [0, 0]])\n"]}],"source":["crow_indices = torch.tensor([0, 2, 2]) # 0이 아닌 행의 위치 (첫번쨰는 무조건 0), 즉 row_pointer\n","col_indices = torch.tensor([0, 1]) # 0이 아닌 열의 위치\n","values = torch.tensor([1, 2]) # 0이 아닌 값\n","csr = torch.sparse_csr_tensor(crow_indices = crow_indices, col_indices = col_indices, values = values)\n","\n","print(csr)\n","print('\\n')\n","print(csr.to_dense())"]},{"cell_type":"markdown","metadata":{"id":"cQRzTsDXs5V0"},"source":["#### 📝 설명 : CSC Sparse Tensor 생성\n","* sparse_csc_tensor : CSC 형식의 Sparse tensor 를 생성하는 함수\n","\n","📚 참고할만한 자료:\n","* [sparse_csc_tensor] : https://pytorch.org/docs/stable/generated/torch.sparse_csc_tensor.html"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1689150854835,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"zZ5iVQMRvEH3","outputId":"d3aaa223-79a3-4f05-ae29-4bca1b3fbff6"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(ccol_indices=tensor([0, 2, 2]),\n","       row_indices=tensor([0, 1]),\n","       values=tensor([1, 2]), size=(2, 2), nnz=2, layout=torch.sparse_csc)\n","\n","\n","tensor([[1, 0],\n","        [2, 0]])\n"]}],"source":["ccol_indices = torch.tensor([0, 2, 2]) # 0이 아닌 열의 위치 (첫번쨰는 무조건 0), 즉 column_pointer\n","row_indices = torch.tensor([0, 1]) # 0이 아닌 행의 위치\n","values = torch.tensor([1, 2]) # 0이 아닌 값\n","csc = torch.sparse_csc_tensor(ccol_indices = ccol_indices, row_indices = row_indices, values = values)\n","\n","print(csc)\n","print('\\n')\n","print(csc.to_dense())"]},{"cell_type":"markdown","metadata":{"id":"4hQgE14lvWJO"},"source":["### 2-3 Sparse Tensor의 필요성 이해 및 실습\n","\n","> Dense tensor가 가지는 한계점에 대해 이해하고, Sparse tensor 가 필요한 이유에 대해 알아봅니다."]},{"cell_type":"markdown","metadata":{"id":"bBeGk-dYxVt0"},"source":["#### 📝 설명 : Sparse Tensor 의 필요성\n","* 아주 큰 크기의 matrix 를 구성할 때, 일반적인 dense tensor 는 메모리 아웃 현상이 발생하지만, sparse tensor 는 메모리 아웃현상이 발생하지 않습니다.\n","  * to_dense() : sparse tensor 를 dense tensor 로 만드는 함수"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"Z-sKmel6o5yk"},"outputs":[],"source":["i = torch.randint(0, 100000, (200000,)).reshape(2, -1)\n","v = torch.rand(100000)\n","coo_sparse_tensor = torch.sparse_coo_tensor(indices = i, values = v, size = [100000, 100000]) # COO Sparse Tensor (100000 x 100000)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1689151188482,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"vIAhKakiuDU6","outputId":"f88a865e-2ed9-485c-b9a7-7670c7b01f82"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["crow = torch.randint(0, 100000, (100000,))\n","col = torch.randint(0, 100000, (100000,))\n","v = torch.rand(100000)\n","csr_sparse_tensor = torch.sparse_csr_tensor(crow_indices = crow, col_indices = col, values = v) # CSR Sparse Tensor (100000 x 100000)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"D2ypY-75p112"},"outputs":[],"source":["coo_sparse_tensor.to_dense() # COO 형식으로 만들어진 Sparse Tensor 를 Dense Tensor 로 변환 , 메모리 아웃"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"T0ZvMku8Qy_f"},"outputs":[],"source":["import torch # 커널 재시작 하기에 다시 torch library 로드"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"gCLXEXERTUJ3"},"outputs":[{"ename":"NameError","evalue":"name 'csr_sparse_tensor' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcsr_sparse_tensor\u001b[49m\u001b[38;5;241m.\u001b[39mto_dense() \u001b[38;5;66;03m# CSR 형식으로 만들어진 Sparse Tensor 를 Dense Tensor 로 변환 , 메모리 아웃\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'csr_sparse_tensor' is not defined"]}],"source":["csr_sparse_tensor.to_dense() # CSR 형식으로 만들어진 Sparse Tensor 를 Dense Tensor 로 변환 , 메모리 아웃"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"5ad4Iu5aQ9dd"},"outputs":[],"source":["import torch # 커널 재시작 하기에 다시 torch library 로드"]},{"cell_type":"markdown","metadata":{"id":"cOYA2R66v46l"},"source":["### 2-4 Sparse Tensor의 조작 방법\n","\n","> Sparse tensor 의 Indexing 과 연산 방법에 대해 알아봅니다."]},{"cell_type":"markdown","metadata":{"id":"tD8jNabnxepy"},"source":["#### 📝 설명 : Sparse Tensor 의 연산 (=2차원)\n","* 2차원 sparse tensor 간에는 일반 텐서와 동일하게 사칙연산 함수들과 행렬곱을 사용할 수 있습니다.\n","\n","📚 참고할만한 자료:\n","* [sparse] : https://pytorch.org/docs/stable/sparse.html"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":322,"status":"ok","timestamp":1689151328420,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"06ZsXbnOv38R","outputId":"dbafb688-bdec-4ede-ce7d-103393d9ade7"},"outputs":[{"name":"stdout","output_type":"stream","text":["덧셈\n","tensor([[True, True],\n","        [True, True]])\n","\n","\n","곱셈\n","tensor([[True, True],\n","        [True, True]])\n","\n","\n","행렬곱\n","tensor([[True, True],\n","        [True, True]])\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_22062/3038429869.py:18: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n","  print(torch.matmul(a, b).to_dense() == torch.matmul(sparse_a, sparse_b).to_dense())\n"]}],"source":["# Sparse 와 Sparse Tensor 간의 연산 (2차원)\n","a = torch.tensor([[0, 1], [0, 2]], dtype=torch.float)\n","b = torch.tensor([[1, 0],[0, 0]], dtype=torch.float)\n","\n","sparse_a = a.to_sparse()\n","sparse_b = b.to_sparse()\n","\n","print('덧셈')\n","print(torch.add(a, b).to_dense() == torch.add(sparse_a, sparse_b).to_dense())\n","\n","print('\\n')\n","print('곱셈')\n","print(torch.mul(a, b).to_dense() == torch.mul(sparse_a, sparse_b).to_dense())\n","\n","print('\\n')\n","\n","print('행렬곱')\n","print(torch.matmul(a, b).to_dense() == torch.matmul(sparse_a, sparse_b).to_dense())"]},{"cell_type":"markdown","metadata":{"id":"QNxWiMSij8A-"},"source":["#### 📝 설명 : Sparse Tensor 의 연산 (=3차원)\n","* 3차원 sparse tensor 에는 일반 텐서와 동일하게 사칙연산 함수들은 사용 가능하지만 행렬곱을 사용할 수 없습니다.\n","  * CSR/CSC 형식에서는 곱셈도 3차원에선 불가능합니다.\n","* 이는 sparse tensor 와 sparse tensor 간에도 적용이 되고, sparse tensor 와 dense tensor 간의 연산에도 적용이 됩니다."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1091,"status":"error","timestamp":1689151417573,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"UZFYPnQGjezX","outputId":"81d642fd-e1ed-4ed7-8b81-4d3b82f5a2e7"},"outputs":[{"name":"stdout","output_type":"stream","text":["덧셈\n","tensor([[[True, True],\n","         [True, True]],\n","\n","        [[True, True],\n","         [True, True]]])\n","\n","\n","곱셈\n","tensor([[[True, True],\n","         [True, True]],\n","\n","        [[True, True],\n","         [True, True]]])\n","\n","\n","행렬곱\n"]},{"ename":"RuntimeError","evalue":"expand is unsupported for Sparse tensors","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m행렬곱\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mmatmul(a, b)\u001b[38;5;241m.\u001b[39mto_dense() \u001b[38;5;241m==\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparse_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_b\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_dense()) \u001b[38;5;66;03m# 에러 발생\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: expand is unsupported for Sparse tensors"]}],"source":["# Sparse 와 Sparse Tensor 간의 연산 (3차원)\n","a = torch.tensor([[[0, 1], [0, 2]], [[0, 1], [0, 2]]], dtype=torch.float)\n","b = torch.tensor([[[1, 0],[0, 0]], [[1, 0], [0, 0]]], dtype=torch.float)\n","\n","sparse_a = a.to_sparse()\n","sparse_b = b.to_sparse()\n","\n","print('덧셈')\n","print(torch.add(a, b).to_dense() == torch.add(sparse_a, sparse_b).to_dense())\n","\n","print('\\n')\n","print('곱셈')\n","print(torch.mul(a, b).to_dense() == torch.mul(sparse_a, sparse_b).to_dense())\n","\n","print('\\n')\n","\n","print('행렬곱')\n","print(torch.matmul(a, b).to_dense() == torch.matmul(sparse_a, sparse_b).to_dense()) # 에러 발생"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":496,"status":"ok","timestamp":1689151468214,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"JkMRIqI4kZUX","outputId":"f241d42b-df52-456e-b81d-245eb5f9bca1"},"outputs":[{"name":"stdout","output_type":"stream","text":["덧셈\n","tensor([[True, True],\n","        [True, True]])\n","\n","\n","곱셈\n","tensor([[True, True],\n","        [True, True]])\n","\n","\n","행렬곱\n","tensor([[True, True],\n","        [True, True]])\n"]}],"source":["# Dense 와 Sparse Tensor 간의 연산 (2차원)\n","a = torch.tensor([[0,1],[0,2]],dtype=torch.float)\n","b = torch.tensor([[1,0],[0,0]],dtype=torch.float).to_sparse()\n","\n","sparse_b = b.to_sparse()\n","\n","print('덧셈')\n","print(torch.add(a, b).to_dense() == torch.add(a, sparse_b).to_dense())\n","\n","print('\\n')\n","print('곱셈')\n","print(torch.mul(a, b).to_dense() == torch.mul(a, sparse_b).to_dense())\n","\n","print('\\n')\n","\n","print('행렬곱')\n","print(torch.matmul(a, b).to_dense() == torch.matmul(a, sparse_b).to_dense())"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":5,"status":"error","timestamp":1688799085394,"user":{"displayName":"김영민","userId":"04915862517565535031"},"user_tz":-540},"id":"kcZEH-sedkpI","outputId":"624ec917-cf3f-47b6-a908-887972fcc304"},"outputs":[{"name":"stdout","output_type":"stream","text":["덧셈\n","tensor([[[True, True],\n","         [True, True]],\n","\n","        [[True, True],\n","         [True, True]]])\n","\n","\n","곱셈\n","tensor([[[True, True],\n","         [True, True]],\n","\n","        [[True, True],\n","         [True, True]]])\n","\n","\n","행렬곱\n"]},{"ename":"RuntimeError","evalue":"expand is unsupported for Sparse tensors","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m행렬곱\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mmatmul(a, b)\u001b[38;5;241m.\u001b[39mto_dense() \u001b[38;5;241m==\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_b\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_dense()) \u001b[38;5;66;03m# 에러 발생\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: expand is unsupported for Sparse tensors"]}],"source":["a = torch.tensor([[[0, 1], [0, 2]], [[0, 1], [0, 2]]],dtype=torch.float)\n","b = torch.tensor([[[1, 0], [0, 0]], [[1, 0], [0, 0]]],dtype=torch.float)\n","\n","sparse_b = b.to_sparse()\n","\n","print('덧셈')\n","print(torch.add(a, b).to_dense() == torch.add(a, sparse_b).to_dense())\n","\n","print('\\n')\n","print('곱셈')\n","print(torch.mul(a, b).to_dense() == torch.mul(a, sparse_b).to_dense())\n","\n","print('\\n')\n","\n","print('행렬곱')\n","print(torch.matmul(a, b).to_dense() == torch.matmul(a, sparse_b).to_dense()) # 에러 발생"]},{"cell_type":"markdown","metadata":{"id":"wR92561My19i"},"source":["#### 📝 설명 : Sparse Tensor 의 Indexing\n","* 일반 텐서와 동일하게 indexing 이 가능합니다.\n","  * slicing (\":\" 을 사용)은 불가능 합니다.\n","  \n","📚 참고할만한 자료:\n","* [sparse] : https://pytorch.org/docs/stable/sparse.html"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":346,"status":"ok","timestamp":1689151604234,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"Hi9s2FBIy7cL","outputId":"8ba32db0-e6e3-4d91-afab-08d57b79388b"},"outputs":[{"name":"stdout","output_type":"stream","text":["2차원 Sparse Tensor 인덱싱\n","tensor([True, True])\n","\n","\n","3차원 Sprase Tensor 인덱싱\n","tensor([[True, True],\n","        [True, True]])\n"]}],"source":["a = torch.tensor([[0,1 ], [0, 2]], dtype=torch.float)\n","b = torch.tensor([[[1, 0], [0, 0]], [[1, 0], [0, 0]]], dtype=torch.float)\n","\n","sparse_a = a.to_sparse()\n","sparse_b = b.to_sparse()\n","\n","print('2차원 Sparse Tensor 인덱싱')\n","print(a[0] == sparse_a[0].to_dense())\n","\n","print('\\n')\n","\n","print('3차원 Sprase Tensor 인덱싱')\n","print(b[0] == sparse_b[0].to_dense())"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":423,"status":"error","timestamp":1689151656029,"user":{"displayName":"Eddie(김윤기)","userId":"11850705511374304262"},"user_tz":-540},"id":"L6Ar7NX9oUkG","outputId":"eff7bdcc-0418-45ca-83ec-f8fd3cb4d395"},"outputs":[{"ename":"NotImplementedError","evalue":"Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31419 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44504 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:26984 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:951 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:22129 [kernel]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nZeroTensor: registered at aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4930 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16910 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:735 [kernel]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto_sparse_csr() \u001b[38;5;66;03m# 2dim Sparse Tensor (CSR)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# 0행의 모든 원소 추출 => 에러 발생\u001b[39;00m\n","\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::as_strided' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::as_strided' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31419 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44504 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:26984 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:951 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:22129 [kernel]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: fallthrough registered at ../aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\nNegative: fallthrough registered at ../aten/src/ATen/native/NegateFallback.cpp:22 [kernel]\nZeroTensor: registered at aten/src/ATen/RegisterZeroTensor.cpp:161 [kernel]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4930 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17438 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16910 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:735 [kernel]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n"]}],"source":["a = torch.tensor([[0, 1], [0, 2]], dtype=torch.float).to_sparse_csr() # 2dim Sparse Tensor (CSR)\n","a[0,:] # 0행의 모든 원소 추출 => 에러 발생"]},{"cell_type":"markdown","metadata":{"id":"lSVO_LtU2erF"},"source":["#Reference\n","> <b><font color = green>(📒가이드)\n","- <a href='https://pytorch.org/docs/stable/index.html'>PyTorch 공식 문서</a>\n","- <a href='https://bkshin.tistory.com/entry/NLP-7-%ED%9D%AC%EC%86%8C-%ED%96%89%EB%A0%AC-Sparse-Matrix-COO-%ED%98%95%EC%8B%9D-CSR-%ED%98%95%EC%8B%9D'>COO 와 CSR/CSC</a>"]},{"cell_type":"markdown","metadata":{"id":"4by_4X6tvaX6"},"source":["## Required Package\n","\n","> torch == 2.0.1"]},{"cell_type":"markdown","metadata":{"id":"-P3G5QSQvbSg"},"source":["## 콘텐츠 라이선스\n","\n","저작권 : <font color='blue'> <b> ©2023 by Upstage X fastcampus Co., Ltd. All rights reserved.</font></b>\n","\n","<font color='red'><b>WARNING</font> : 본 교육 콘텐츠의 지식재산권은 업스테이지 및 패스트캠퍼스에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다. </b>"]}],"metadata":{"colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
